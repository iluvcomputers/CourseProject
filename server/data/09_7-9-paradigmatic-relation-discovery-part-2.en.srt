1
00:00:05,960 --> 00:00:08,625
In this lecture, we continue

2
00:00:08,625 --> 00:00:11,565
discussing Paradigmatical
Relation Discovery.

3
00:00:11,565 --> 00:00:14,175
Earlier we introduced
a method called

4
00:00:14,175 --> 00:00:16,935
Expected Overlap of
Words in Context.

5
00:00:16,935 --> 00:00:21,090
In this method, we
represent each context by

6
00:00:21,090 --> 00:00:23,040
a word vector that represents

7
00:00:23,040 --> 00:00:26,490
the probability of a
word in the context.

8
00:00:26,490 --> 00:00:30,345
We measure the similarity
by using the.product,

9
00:00:30,345 --> 00:00:34,320
which can be interpreted as
the probability that two

10
00:00:34,320 --> 00:00:36,240
randomly picked words from

11
00:00:36,240 --> 00:00:38,585
the two contexts are identical.

12
00:00:38,585 --> 00:00:42,515
We also discussed
the two problems of this method.

13
00:00:42,515 --> 00:00:45,920
The first is that
it favors matching

14
00:00:45,920 --> 00:00:47,900
one frequent term very well over

15
00:00:47,900 --> 00:00:50,390
matching more distinct terms.

16
00:00:50,390 --> 00:00:55,235
It put too much emphasis on
matching one term very well.

17
00:00:55,235 --> 00:01:00,350
The second is that it
treats every word equally.

18
00:01:00,350 --> 00:01:03,995
Even a common word like
the will contribute

19
00:01:03,995 --> 00:01:08,885
equally as content
word like eats.

20
00:01:08,885 --> 00:01:11,270
So now we are

21
00:01:11,270 --> 00:01:13,715
going to talk about how
to solve these problems.

22
00:01:13,715 --> 00:01:15,965
More specifically, we're
going to introduce

23
00:01:15,965 --> 00:01:19,790
some retrieval heuristics
used in text retrieval.

24
00:01:19,790 --> 00:01:23,900
These heuristics can effectively
solve these problems,

25
00:01:23,900 --> 00:01:26,975
as these problems also
occur in text retrieval

26
00:01:26,975 --> 00:01:30,680
when we match a query that
though with a document vector.

27
00:01:30,680 --> 00:01:32,920
So to address the first problem,

28
00:01:32,920 --> 00:01:36,385
we can use a sublinear
transformation of tone frequency.

29
00:01:36,385 --> 00:01:37,970
That is, we don't have to use

30
00:01:37,970 --> 00:01:39,650
the raw frequency count of

31
00:01:39,650 --> 00:01:42,140
a term to represent the context.

32
00:01:42,140 --> 00:01:44,780
We can transform
it into some form

33
00:01:44,780 --> 00:01:48,025
that wouldn't emphasize so
much on the raw frequency.

34
00:01:48,025 --> 00:01:50,130
To address the
synchronous problem,

35
00:01:50,130 --> 00:01:53,195
we can put more weight
on rare terms.

36
00:01:53,195 --> 00:01:56,330
That is we can reward
matching a real-world.

37
00:01:56,330 --> 00:01:58,760
This heuristic is called the IDF

38
00:01:58,760 --> 00:02:01,135
term weighting in text retrieval.

39
00:02:01,135 --> 00:02:05,085
IDF stands for
Inverse Document Frequency.

40
00:02:05,085 --> 00:02:07,010
So now, we're going to talk about

41
00:02:07,010 --> 00:02:10,130
the two heuristics
in more detail.

42
00:02:10,130 --> 00:02:13,930
First let's talk about
the TF Transformation.

43
00:02:13,930 --> 00:02:16,400
That is to convert
the raw count of

44
00:02:16,400 --> 00:02:19,565
a word in the document
into some weight

45
00:02:19,565 --> 00:02:23,195
that reflects our belief

46
00:02:23,195 --> 00:02:27,200
about how important
this word in the document.

47
00:02:27,200 --> 00:02:32,370
So that will be
denoted by TF of w,d.

48
00:02:32,370 --> 00:02:36,415
That's shown in the y-axis.

49
00:02:36,415 --> 00:02:39,920
Now, in general, there are
many ways to map that.

50
00:02:39,920 --> 00:02:44,250
Let's first look at
the simple way of mapping.

51
00:02:44,250 --> 00:02:47,920
In this case, we're
going to say, well,

52
00:02:47,920 --> 00:02:51,510
any non-zero counts
will be mapped to

53
00:02:51,510 --> 00:02:55,450
one and the zero count
will be mapped to zero.

54
00:02:55,450 --> 00:02:56,990
So with this mapping

55
00:02:56,990 --> 00:02:59,240
all the frequencies will be

56
00:02:59,240 --> 00:03:02,605
mapped to only two
values; zero or one.

57
00:03:02,605 --> 00:03:11,015
The mapping function is shown
here as a flat line here.

58
00:03:11,015 --> 00:03:14,030
Now, this is naive

59
00:03:14,030 --> 00:03:16,660
because it's not
the frequency of words.

60
00:03:16,660 --> 00:03:20,195
However, this actually
has the advantage of

61
00:03:20,195 --> 00:03:25,700
emphasizing matching all
the words in the context.

62
00:03:25,700 --> 00:03:27,725
So it does not allow

63
00:03:27,725 --> 00:03:30,505
a frequency of word to
dominate the matching.

64
00:03:30,505 --> 00:03:32,930
Now, the approach
that we have taken

65
00:03:32,930 --> 00:03:36,650
earlier in the expected
overlap count approach,

66
00:03:36,650 --> 00:03:38,225
is a linear transformation.

67
00:03:38,225 --> 00:03:41,870
We basically, take
y as the same as x.

68
00:03:41,870 --> 00:03:45,445
So we use the raw count
as a representation.

69
00:03:45,445 --> 00:03:48,140
That created the problem

70
00:03:48,140 --> 00:03:50,360
that we just talked about namely;

71
00:03:50,360 --> 00:03:54,935
it emphasize too much on just
matching one frequent term.

72
00:03:54,935 --> 00:03:58,520
Matching one frequent term
can contribute a lot.

73
00:03:58,520 --> 00:04:02,750
So we can have a lot

74
00:04:02,750 --> 00:04:04,475
of other interesting
transformations

75
00:04:04,475 --> 00:04:06,875
in between the two extremes,

76
00:04:06,875 --> 00:04:10,640
and they generally form
a sublinear transformation.

77
00:04:10,640 --> 00:04:13,340
So for example,
one possibility is to take

78
00:04:13,340 --> 00:04:16,080
logarithm of the raw count,

79
00:04:16,080 --> 00:04:19,400
and this will give us curve
that looks like this,

80
00:04:19,400 --> 00:04:21,260
that you are seeing here.

81
00:04:21,260 --> 00:04:25,295
In this case, you can see
the high frequency counts.

82
00:04:25,295 --> 00:04:29,330
The high counts are
penalize a little bit,

83
00:04:29,330 --> 00:04:33,470
so the curve is a sublinear
curve and it brings down

84
00:04:33,470 --> 00:04:39,240
the weight of
those really high counts.

85
00:04:39,240 --> 00:04:42,875
This is what we want,
because it prevents that

86
00:04:42,875 --> 00:04:47,340
terms from dominating
the scoring function.

87
00:04:48,620 --> 00:04:50,870
Now, there is also

88
00:04:50,870 --> 00:04:52,760
another interesting
transformation called

89
00:04:52,760 --> 00:04:55,430
a BM25 transformation which

90
00:04:55,430 --> 00:04:59,945
has been shown to be very
effective for retrieval.

91
00:04:59,945 --> 00:05:02,735
In this transformation, we have

92
00:05:02,735 --> 00:05:07,225
a form that looks like this.

93
00:05:07,225 --> 00:05:11,640
So it's k plus one multiplied
by x divided by x plus k,

94
00:05:11,640 --> 00:05:13,800
where k is a parameter,

95
00:05:13,800 --> 00:05:16,485
x is the count,

96
00:05:16,485 --> 00:05:18,690
the raw count of a word.

97
00:05:18,690 --> 00:05:22,190
Now, the transformation
is very interesting in

98
00:05:22,190 --> 00:05:25,430
that it can actually go from

99
00:05:25,430 --> 00:05:28,910
one extreme to the other
extreme by varying

100
00:05:28,910 --> 00:05:34,725
k. It also interesting
that it has upper bound,

101
00:05:34,725 --> 00:05:37,135
k plus one in this case.

102
00:05:37,135 --> 00:05:41,435
So this puts
a very strict constraint

103
00:05:41,435 --> 00:05:43,040
on high frequency terms,

104
00:05:43,040 --> 00:05:46,460
because their weight would
never exceed k plus one.

105
00:05:46,460 --> 00:05:50,900
As we vary k, if we can
simulate the two extremes.

106
00:05:50,900 --> 00:05:52,590
So when k is set to zero,

107
00:05:52,590 --> 00:05:55,680
we roughly have the 0,1 vector.

108
00:05:55,680 --> 00:05:59,090
Whereas when we set k
to a very large value,

109
00:05:59,090 --> 00:06:02,075
it will behave more like
the linear transformation.

110
00:06:02,075 --> 00:06:05,270
So this transformation
function is by

111
00:06:05,270 --> 00:06:07,880
far the most effective
transformation function for

112
00:06:07,880 --> 00:06:10,880
text retrieval and it also makes

113
00:06:10,880 --> 00:06:14,285
sense for our problem setup.

114
00:06:14,285 --> 00:06:17,195
So we just talked about how
to solve the problem of

115
00:06:17,195 --> 00:06:20,660
overemphasizing a frequency term

116
00:06:20,660 --> 00:06:22,850
Now let's look at
the second problem,

117
00:06:22,850 --> 00:06:26,585
and that is how we can
penalize popular terms.

118
00:06:26,585 --> 00:06:28,935
Matching "the" is not surprising,

119
00:06:28,935 --> 00:06:30,645
because "the" occurs everywhere.

120
00:06:30,645 --> 00:06:33,020
But matching "eats"
would count a lot.

121
00:06:33,020 --> 00:06:35,105
So how can we address
that problem?

122
00:06:35,105 --> 00:06:38,965
Now in this case, we can
use the IDF weighting.

123
00:06:38,965 --> 00:06:42,095
That's commonly
used in retrieval.

124
00:06:42,095 --> 00:06:45,065
IDF stands for
Inverse Document Frequency.

125
00:06:45,065 --> 00:06:47,675
Document frequency
means the count

126
00:06:47,675 --> 00:06:49,370
of the total number of

127
00:06:49,370 --> 00:06:52,235
documents that contain
a particular word.

128
00:06:52,235 --> 00:06:57,200
So here we show that the IDF
measure is defined as

129
00:06:57,200 --> 00:07:00,230
a logarithm function
of the number

130
00:07:00,230 --> 00:07:05,065
of documents that match a
term or document frequency.

131
00:07:05,065 --> 00:07:08,870
So K is the number of
documents containing word or

132
00:07:08,870 --> 00:07:11,630
document frequency and M

133
00:07:11,630 --> 00:07:14,615
here is the total number of
documents in the collection.

134
00:07:14,615 --> 00:07:21,200
The IDF function is giving
a higher value for a lower K,

135
00:07:21,200 --> 00:07:24,815
meaning that it
rewards rare term.

136
00:07:24,815 --> 00:07:28,805
The maximum value is
log of M plus one.

137
00:07:28,805 --> 00:07:33,650
That's when the word occurred
just once in a context.

138
00:07:33,650 --> 00:07:37,235
So that's a very rare term,

139
00:07:37,235 --> 00:07:40,745
the rare is term in
the whole collection.

140
00:07:40,745 --> 00:07:46,700
The lowest value you can
see here is when K reaches

141
00:07:46,700 --> 00:07:49,115
its maximum which would be M.

142
00:07:49,115 --> 00:07:53,880
So that would be
a very low value,

143
00:07:53,990 --> 00:07:57,340
close to zero in fact.

144
00:07:57,470 --> 00:08:02,360
So this of course measure

145
00:08:02,360 --> 00:08:06,740
is used in search where we
naturally have a collection.

146
00:08:06,740 --> 00:08:09,960
In our case, what would
be our collection?

147
00:08:09,960 --> 00:08:13,040
Well, we can also
use the context that

148
00:08:13,040 --> 00:08:16,610
we can collect all the words
as our collection.

149
00:08:16,610 --> 00:08:18,590
That is to say,

150
00:08:18,590 --> 00:08:22,225
a word that's popular in
the collection in general,

151
00:08:22,225 --> 00:08:25,650
would also have a low IDF.

152
00:08:25,650 --> 00:08:29,445
Because depending on the dataset,

153
00:08:29,445 --> 00:08:35,105
we can construct the context
vectors in different ways.

154
00:08:35,105 --> 00:08:38,010
But in the end if a term is

155
00:08:38,010 --> 00:08:41,024
very frequent in
the original dataset,

156
00:08:41,024 --> 00:08:43,210
then it will still be frequent

157
00:08:43,210 --> 00:08:47,220
in the collective
context documents.

158
00:08:47,620 --> 00:08:52,355
So how can we add
these heuristics to

159
00:08:52,355 --> 00:08:56,910
improve our similarity function?

160
00:08:56,910 --> 00:08:58,565
Well, here's one way
and there are

161
00:08:58,565 --> 00:09:00,920
many other ways
that are possible.

162
00:09:00,920 --> 00:09:02,520
But this is a reasonable way,

163
00:09:02,520 --> 00:09:05,825
where we can adapt
the BM25 retrieval model

164
00:09:05,825 --> 00:09:09,520
for paradigmatical
relation mining.

165
00:09:14,120 --> 00:09:20,555
In this case, we define the
document vector as containing

166
00:09:20,555 --> 00:09:26,825
elements representing
normalized BM25 values.

167
00:09:26,825 --> 00:09:29,810
So in this
normalization function,

168
00:09:29,810 --> 00:09:36,985
we take sum over all
the words and we

169
00:09:36,985 --> 00:09:42,155
normalize the weight of
each word by the sum

170
00:09:42,155 --> 00:09:48,210
of the weights of all the words.

171
00:09:48,210 --> 00:09:51,030
This is to again ensure all the

172
00:09:51,030 --> 00:09:53,975
xi's will sum to
one in this vector.

173
00:09:53,975 --> 00:09:57,800
So this would be very similar
to what we had before,

174
00:09:57,800 --> 00:09:59,420
in that this vector is

175
00:09:59,420 --> 00:10:04,015
actually something similar
to a word distribution,

176
00:10:04,015 --> 00:10:06,685
all the xi's will sum to one.

177
00:10:06,685 --> 00:10:13,560
Now, the weight of BM25 for
each word is defined here.

178
00:10:14,460 --> 00:10:18,940
If you compare this with
our old definition where we just

179
00:10:18,940 --> 00:10:22,930
have a normalized count
of this one, right?

180
00:10:22,930 --> 00:10:26,320
So we only have this one
and the document lens or

181
00:10:26,320 --> 00:10:31,090
the total counts of words in
that context to document,

182
00:10:31,090 --> 00:10:33,430
and that's what we had before.

183
00:10:33,430 --> 00:10:36,039
But now with the BM25
transformation,

184
00:10:36,039 --> 00:10:38,335
we introduced something else.

185
00:10:38,335 --> 00:10:42,040
First, of course,
this extra occurrence of

186
00:10:42,040 --> 00:10:43,420
this count is just to

187
00:10:43,420 --> 00:10:46,075
achieve the sub-linear
normalization.

188
00:10:46,075 --> 00:10:50,155
But we also see we introduced
the parameter, k, here,

189
00:10:50,155 --> 00:10:56,110
and this parameter is
generally a non-active number,

190
00:10:56,110 --> 00:10:58,810
although zero is also possible.

191
00:10:58,810 --> 00:11:02,950
But this controls
the upper bound,

192
00:11:02,950 --> 00:11:06,535
and also controls to what extent

193
00:11:06,535 --> 00:11:11,240
it simulates the
linear transformation.

194
00:11:11,250 --> 00:11:14,830
So this is one parameter,

195
00:11:14,830 --> 00:11:17,140
but we also see there is
another parameter here,

196
00:11:17,140 --> 00:11:21,115
b, and this would be
within zero and one.

197
00:11:21,115 --> 00:11:25,405
This is a parameter to
control lens normalization.

198
00:11:25,405 --> 00:11:27,294
In this case,

199
00:11:27,294 --> 00:11:29,200
the normalization formula has

200
00:11:29,200 --> 00:11:31,885
a average document lens here.

201
00:11:31,885 --> 00:11:35,770
This is computed up
by taking the average

202
00:11:35,770 --> 00:11:39,880
of the lenses of all the
documents in the collection.

203
00:11:39,880 --> 00:11:41,605
In this case, all the lenses of

204
00:11:41,605 --> 00:11:45,340
all the context of documents
that we're considering.

205
00:11:45,340 --> 00:11:48,175
So this average documents

206
00:11:48,175 --> 00:11:50,425
will be a constant for
any given collection.

207
00:11:50,425 --> 00:11:52,795
So it actually is only

208
00:11:52,795 --> 00:11:56,530
affecting the effect
of the parameter,

209
00:11:56,530 --> 00:12:01,180
b, here because
this is a constant.

210
00:12:01,180 --> 00:12:07,780
But I kept it here because
it's a constant that's used

211
00:12:07,780 --> 00:12:10,840
for in retrieval where it would

212
00:12:10,840 --> 00:12:14,770
give us a stabilized
interpretation of parameter, b.

213
00:12:14,770 --> 00:12:16,570
But for our purpose,

214
00:12:16,570 --> 00:12:21,430
this will be a constant so
it would only be affecting

215
00:12:21,430 --> 00:12:28,550
the lens normalization
together with parameter, b.

216
00:12:29,400 --> 00:12:33,295
Now, with this definition then,

217
00:12:33,295 --> 00:12:37,810
we have a new way to define
our document of vectors,

218
00:12:37,810 --> 00:12:41,785
and we can compute
the vector d2 in the same way.

219
00:12:41,785 --> 00:12:43,255
The difference is that

220
00:12:43,255 --> 00:12:44,950
the high-frequency terms will now

221
00:12:44,950 --> 00:12:46,930
have a somewhat lower weights.

222
00:12:46,930 --> 00:12:49,690
This would help us control

223
00:12:49,690 --> 00:12:53,575
the inference of
these high-frequency terms.

224
00:12:53,575 --> 00:12:58,000
Now, the idea can be added
here in the scoring function.

225
00:12:58,000 --> 00:12:59,905
That means we'll
introduce a weight

226
00:12:59,905 --> 00:13:01,990
for matching each term.

227
00:13:01,990 --> 00:13:05,650
So you may recall
this sum indicates

228
00:13:05,650 --> 00:13:08,305
all the possible words
that can be

229
00:13:08,305 --> 00:13:11,365
overlap between the two contexts.

230
00:13:11,365 --> 00:13:15,790
The x_i and the y_i
are probabilities

231
00:13:15,790 --> 00:13:20,245
of picking the word
from both contexts.

232
00:13:20,245 --> 00:13:22,330
Therefore, it
indicates how likely

233
00:13:22,330 --> 00:13:24,805
we'll see a match on this word.

234
00:13:24,805 --> 00:13:26,695
Now, IDF would give us

235
00:13:26,695 --> 00:13:29,200
the importance of
matching this word.

236
00:13:29,200 --> 00:13:33,700
A common word will be worth
less than a rare word.

237
00:13:33,700 --> 00:13:36,715
So we emphasize more on
matching rare words now.

238
00:13:36,715 --> 00:13:38,785
So with this modification,

239
00:13:38,785 --> 00:13:40,660
then the new function will

240
00:13:40,660 --> 00:13:43,270
likely address
those two problems.

241
00:13:43,270 --> 00:13:45,310
Now, interestingly
we can also use

242
00:13:45,310 --> 00:13:49,825
this approach to discover
syntagmatic relations.

243
00:13:49,825 --> 00:13:57,430
In general, when we re-brand

244
00:13:57,430 --> 00:13:59,365
a context with a term vector,

245
00:13:59,365 --> 00:14:01,900
we would likely see

246
00:14:01,900 --> 00:14:04,135
some terms have high weights

247
00:14:04,135 --> 00:14:06,040
and other terms have low weights.

248
00:14:06,040 --> 00:14:09,490
Depending on how we assign
weights to these terms,

249
00:14:09,490 --> 00:14:11,650
we might be able to
use these weights to

250
00:14:11,650 --> 00:14:13,720
discover the words that

251
00:14:13,720 --> 00:14:15,700
are strongly associated with

252
00:14:15,700 --> 00:14:18,400
the candidate word
in the context.

253
00:14:18,400 --> 00:14:20,560
So let's take a look at

254
00:14:20,560 --> 00:14:23,815
the term vector in
more detail here.

255
00:14:23,815 --> 00:14:29,885
We have each x_i

256
00:14:29,885 --> 00:14:33,610
defined as the normalized
weight of BM25.

257
00:14:33,610 --> 00:14:37,420
Now, this weight alone only

258
00:14:37,420 --> 00:14:41,110
reflects how frequent the word
occurs in the context.

259
00:14:41,110 --> 00:14:43,345
But we can't just say

260
00:14:43,345 --> 00:14:44,500
any frequent term in

261
00:14:44,500 --> 00:14:46,560
the context that would
be correlated with

262
00:14:46,560 --> 00:14:50,235
the candidate word because

263
00:14:50,235 --> 00:14:51,990
many common words like 'the' will

264
00:14:51,990 --> 00:14:54,540
occur frequently in
all the context.

265
00:14:54,540 --> 00:14:59,645
But if we apply IDF
weighting as you see here,

266
00:14:59,645 --> 00:15:07,090
we can then re-weight
these terms based on IDF.

267
00:15:07,090 --> 00:15:09,220
That means the words that are

268
00:15:09,220 --> 00:15:11,920
common like 'the'
will get penalized.

269
00:15:11,920 --> 00:15:14,920
So now the highest
weighted terms will not be

270
00:15:14,920 --> 00:15:18,220
those common terms because
they have lower IDFs.

271
00:15:18,220 --> 00:15:20,980
Instead, those terms would

272
00:15:20,980 --> 00:15:23,920
be the terms that are
frequent in the context,

273
00:15:23,920 --> 00:15:26,080
but not frequent
in the collection.

274
00:15:26,080 --> 00:15:29,590
So those are clearly the words
that tend to occur in

275
00:15:29,590 --> 00:15:33,820
the context of the candidate
word, for example, cat.

276
00:15:33,820 --> 00:15:35,365
So for this reason,

277
00:15:35,365 --> 00:15:39,865
the highly weighted terms in
this idea of weighted vector

278
00:15:39,865 --> 00:15:42,310
can also be assumed to

279
00:15:42,310 --> 00:15:45,940
be candidates for
syntagmatic relations.

280
00:15:45,940 --> 00:15:48,895
Now, of course, this is
only a by-product of

281
00:15:48,895 --> 00:15:53,560
our approach for discovering
paradigmatic relations.

282
00:15:53,560 --> 00:15:57,025
In the next lecture, we're
going to talk more about

283
00:15:57,025 --> 00:16:01,850
how to discover
syntagmatic relations.

284
00:16:02,280 --> 00:16:05,305
But it clearly shows the relation

285
00:16:05,305 --> 00:16:08,995
between discovering
the two relations.

286
00:16:08,995 --> 00:16:12,670
Indeed they can be discovered in

287
00:16:12,670 --> 00:16:18,340
a joint manner by leveraging
such associations.

288
00:16:18,340 --> 00:16:22,600
So to summarize,
the main idea for discovering

289
00:16:22,600 --> 00:16:26,050
paradigmatic relations is to

290
00:16:26,050 --> 00:16:27,610
collect the context of

291
00:16:27,610 --> 00:16:30,460
a candidate word to
form a pseudo document.

292
00:16:30,460 --> 00:16:33,685
This is typically represented
as a bag of words.

293
00:16:33,685 --> 00:16:35,890
Then compute the similarity of

294
00:16:35,890 --> 00:16:38,005
the corresponding
context documents

295
00:16:38,005 --> 00:16:40,540
of two candidate words.

296
00:16:40,540 --> 00:16:45,910
Then we can take
the highly similar word pairs,

297
00:16:45,910 --> 00:16:50,305
and treat them as having
paradigmatic relations.

298
00:16:50,305 --> 00:16:53,395
These are the words that
share similar contexts.

299
00:16:53,395 --> 00:16:55,540
There are many different ways to

300
00:16:55,540 --> 00:16:58,090
implement this general idea.

301
00:16:58,090 --> 00:17:01,435
We just talked about
some of the approaches.

302
00:17:01,435 --> 00:17:04,510
More specifically, we
talked about using

303
00:17:04,510 --> 00:17:07,765
text retrieval models to help us

304
00:17:07,765 --> 00:17:10,690
design effective
similarity function to

305
00:17:10,690 --> 00:17:15,170
compute the
paradigmatic relations.

306
00:17:15,960 --> 00:17:19,330
More specifically, we have used

307
00:17:19,330 --> 00:17:23,020
the BM25 and IDF weighting

308
00:17:23,020 --> 00:17:27,250
to discover
paradigmatic relation.

309
00:17:27,250 --> 00:17:30,100
These approaches also represent

310
00:17:30,100 --> 00:17:33,310
the state of the art in
text retrieval techniques.

311
00:17:33,310 --> 00:17:37,165
Finally, syntagmatic relations
can also be discovered

312
00:17:37,165 --> 00:17:42,140
as a by-product when we discover
paradigmatic relations.