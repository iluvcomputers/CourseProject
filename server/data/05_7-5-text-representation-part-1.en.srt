1
00:00:06,440 --> 00:00:11,320
This lecture is about the
textual representation.

2
00:00:12,020 --> 00:00:14,100
In this lecture, we are going

3
00:00:14,100 --> 00:00:16,680
to discuss textual
representation,

4
00:00:16,680 --> 00:00:20,475
and discuss how natural
language processing can

5
00:00:20,475 --> 00:00:24,450
allow us to represent text
in many different ways.

6
00:00:24,450 --> 00:00:28,590
Let's take a look at this
example sentence again.

7
00:00:28,590 --> 00:00:33,900
We can represent this sentence
in many different ways.

8
00:00:33,900 --> 00:00:37,680
First, we can always

9
00:00:37,680 --> 00:00:42,090
represent such a sentence
as a string of characters.

10
00:00:42,090 --> 00:00:45,135
This is true for
all the languages

11
00:00:45,135 --> 00:00:49,210
when we store them
in the computer.

12
00:00:49,310 --> 00:00:53,480
When we store a natural
language sentence

13
00:00:53,480 --> 00:00:55,805
as a string of characters,

14
00:00:55,805 --> 00:01:00,350
we have perhaps the most general
way of representing text

15
00:01:00,350 --> 00:01:02,270
since we always use

16
00:01:02,270 --> 00:01:05,360
this approach to
represent any text data.

17
00:01:05,360 --> 00:01:10,070
But unfortunately, using
such a representation will not

18
00:01:10,070 --> 00:01:12,950
help us to do semantic analysis,

19
00:01:12,950 --> 00:01:14,135
which is often needed

20
00:01:14,135 --> 00:01:17,600
for many applications
of text mining.

21
00:01:17,600 --> 00:01:21,650
The reason is because we're
not even recognizing words.

22
00:01:21,650 --> 00:01:22,880
So as a string,

23
00:01:22,880 --> 00:01:25,100
we're going to keep
all the spaces

24
00:01:25,100 --> 00:01:29,005
and these ASCII symbols.

25
00:01:29,005 --> 00:01:32,090
We can perhaps count what's

26
00:01:32,090 --> 00:01:35,225
the most frequent character
in English text,

27
00:01:35,225 --> 00:01:38,960
or the correlation
between those characters,

28
00:01:38,960 --> 00:01:43,075
but we can't really
analyze semantics.

29
00:01:43,075 --> 00:01:47,300
Yet, this is the most
general way of representing

30
00:01:47,300 --> 00:01:49,250
text because we can use

31
00:01:49,250 --> 00:01:53,045
this to represent any
natural language text.

32
00:01:53,045 --> 00:01:55,220
If we try to do

33
00:01:55,220 --> 00:01:57,635
a little bit more natural
language processing

34
00:01:57,635 --> 00:02:00,540
by doing word segmentation,

35
00:02:00,540 --> 00:02:05,210
then we can obtain a
representation of the same text,

36
00:02:05,210 --> 00:02:08,315
but in the form of a
sequence of words.

37
00:02:08,315 --> 00:02:11,750
So here we see that
we can identify

38
00:02:11,750 --> 00:02:17,100
words like a dog is chasing etc.

39
00:02:17,230 --> 00:02:20,600
Now with this level
of representation,

40
00:02:20,600 --> 00:02:23,965
we certainly can do
a lot of things,

41
00:02:23,965 --> 00:02:27,065
and this is mainly because
words are the basic units

42
00:02:27,065 --> 00:02:30,275
of human communication
in natural language,

43
00:02:30,275 --> 00:02:33,035
so they are very powerful.

44
00:02:33,035 --> 00:02:36,080
By identifying words, we can for

45
00:02:36,080 --> 00:02:38,780
example easily count what are

46
00:02:38,780 --> 00:02:40,820
the most frequent words in

47
00:02:40,820 --> 00:02:45,185
this document or in
the whole collection etc.

48
00:02:45,185 --> 00:02:48,200
These words can be used to form

49
00:02:48,200 --> 00:02:51,940
topics when we combine
related words together,

50
00:02:51,940 --> 00:02:54,060
and some words are positive,

51
00:02:54,060 --> 00:02:55,700
some words negative, so we can

52
00:02:55,700 --> 00:02:58,410
also do sentiment analysis.

53
00:02:58,660 --> 00:03:02,690
So representing text data
as a sequence of words

54
00:03:02,690 --> 00:03:06,775
opens up a lot of interesting
analysis possibilities.

55
00:03:06,775 --> 00:03:09,060
However, this level of

56
00:03:09,060 --> 00:03:12,080
representation is slightly
less general than string

57
00:03:12,080 --> 00:03:17,300
of characters because in
some languages such as Chinese,

58
00:03:17,300 --> 00:03:21,350
it's actually not
that easy to identify

59
00:03:21,350 --> 00:03:25,010
all the word boundaries
because in such a language,

60
00:03:25,010 --> 00:03:27,685
you see text as a sequence of

61
00:03:27,685 --> 00:03:31,345
characters with
no space in between.

62
00:03:31,345 --> 00:03:33,160
So you'll have to rely on

63
00:03:33,160 --> 00:03:36,925
some special techniques
to identify words.

64
00:03:36,925 --> 00:03:39,940
In such a language,
of course then,

65
00:03:39,940 --> 00:03:43,600
we might make mistakes
in segmenting words.

66
00:03:43,600 --> 00:03:46,480
So the sequence of
words representation is

67
00:03:46,480 --> 00:03:50,230
not as robust as
string of characters.

68
00:03:50,230 --> 00:03:53,230
But in English, it's very

69
00:03:53,230 --> 00:03:56,005
easy to obtain this level
of representation,

70
00:03:56,005 --> 00:03:59,270
so we can do that all the time.

71
00:04:00,860 --> 00:04:03,295
Now, if we go further

72
00:04:03,295 --> 00:04:04,645
to do naturally
language processing,

73
00:04:04,645 --> 00:04:08,125
we can add a part of speech tags.

74
00:04:08,125 --> 00:04:09,955
Now once we do that,

75
00:04:09,955 --> 00:04:11,850
we can count, for example,

76
00:04:11,850 --> 00:04:14,680
the most frequent
nouns or what kind of

77
00:04:14,680 --> 00:04:18,220
nouns are associated with
what kind of verbs etc.

78
00:04:18,220 --> 00:04:19,480
So this opens up

79
00:04:19,480 --> 00:04:23,020
a little bit more
interesting opportunities

80
00:04:23,020 --> 00:04:24,625
for further analysis.

81
00:04:24,625 --> 00:04:28,270
Note that I use a plus sign
here because by

82
00:04:28,270 --> 00:04:32,305
representing text as a sequence
of part of speech tags,

83
00:04:32,305 --> 00:04:35,395
we don't necessarily replace

84
00:04:35,395 --> 00:04:37,840
the original word
sequence written.

85
00:04:37,840 --> 00:04:40,975
Instead, we add this as

86
00:04:40,975 --> 00:04:44,050
an additional way of
representing text data,

87
00:04:44,050 --> 00:04:47,230
so that now the data is
represented as both a sequence

88
00:04:47,230 --> 00:04:50,965
of words and a sequence
of part of speech tags.

89
00:04:50,965 --> 00:04:54,055
This enriches the
representation of text data,

90
00:04:54,055 --> 00:04:59,160
and thus also enables
more interesting analysis.

91
00:05:00,340 --> 00:05:04,040
If we go further, then we'll
be pausing the sentence

92
00:05:04,040 --> 00:05:08,020
often to obtain
a syntactic structure.

93
00:05:08,020 --> 00:05:09,700
Now this of course,

94
00:05:09,700 --> 00:05:12,230
further open up
a more interesting analysis

95
00:05:12,230 --> 00:05:14,840
of, for example,

96
00:05:14,840 --> 00:05:22,520
the writing styles or
correcting grammar mistakes.

97
00:05:22,520 --> 00:05:26,440
If we go further for
semantic analysis,

98
00:05:26,440 --> 00:05:31,684
then we might be able to
recognize dog as an animal,

99
00:05:31,684 --> 00:05:35,515
and we also can recognize
a boy as a person,

100
00:05:35,515 --> 00:05:38,055
and playground as a location.

101
00:05:38,055 --> 00:05:41,335
We can further analyze
their relations, for example,

102
00:05:41,335 --> 00:05:45,830
dog is chasing the boy and
the boy is on the playground.

103
00:05:45,830 --> 00:05:48,875
Now this will add
more entities and

104
00:05:48,875 --> 00:05:52,945
relations through
entity relation recreation.

105
00:05:52,945 --> 00:05:54,790
At this level,

106
00:05:54,790 --> 00:05:57,605
then we can do even more
interesting things.

107
00:05:57,605 --> 00:05:59,795
For example, now we
can count easily

108
00:05:59,795 --> 00:06:02,360
the most frequent person that's

109
00:06:02,360 --> 00:06:06,284
mentioning this whole collection
of news articles,

110
00:06:06,284 --> 00:06:09,205
or whenever you
mention this person,

111
00:06:09,205 --> 00:06:13,655
you also tend to see mentioning
of another person etc.

112
00:06:13,655 --> 00:06:19,480
So this is a very
useful representation,

113
00:06:19,480 --> 00:06:21,830
and it's also related to

114
00:06:21,830 --> 00:06:24,500
the knowledge graph that
some of you may have heard

115
00:06:24,500 --> 00:06:27,620
of that Google is doing as

116
00:06:27,620 --> 00:06:31,690
a more semantic way of
representing text data.

117
00:06:31,690 --> 00:06:39,080
However, it's also less robust
than sequence of words or

118
00:06:39,080 --> 00:06:42,410
even syntactical analysis
because it's not

119
00:06:42,410 --> 00:06:43,985
always easy to identify

120
00:06:43,985 --> 00:06:46,160
all the entities with
the right types,

121
00:06:46,160 --> 00:06:47,735
and we might make mistakes,

122
00:06:47,735 --> 00:06:50,270
and relations are
even harder to find,

123
00:06:50,270 --> 00:06:52,685
and we might make mistakes.

124
00:06:52,685 --> 00:06:56,120
So this makes this level of
representation less robust,

125
00:06:56,120 --> 00:06:58,190
yet it's very useful.

126
00:06:58,190 --> 00:07:01,700
Now if we move further
to logical condition,

127
00:07:01,700 --> 00:07:05,465
then we can have predicates
and even inference rules.

128
00:07:05,465 --> 00:07:08,630
With inference rules, we can

129
00:07:08,630 --> 00:07:13,700
infer interesting derived
facts from the text,

130
00:07:13,700 --> 00:07:15,020
so that's very useful.

131
00:07:15,020 --> 00:07:17,420
But unfortunately,
at this level of

132
00:07:17,420 --> 00:07:19,940
representation is even less

133
00:07:19,940 --> 00:07:22,010
robust and we can make

134
00:07:22,010 --> 00:07:25,120
mistakes and we can't do

135
00:07:25,120 --> 00:07:28,885
that all the time for
all kinds of sentences.

136
00:07:28,885 --> 00:07:33,820
Finally, speech acts would
add a yet another level

137
00:07:33,820 --> 00:07:38,605
of repetition of the intent
of saying this sentence.

138
00:07:38,605 --> 00:07:40,000
So in this case,

139
00:07:40,000 --> 00:07:41,485
it might be a request.

140
00:07:41,485 --> 00:07:44,650
So knowing that would
allow us to analyze

141
00:07:44,650 --> 00:07:47,140
even more interesting
things about

142
00:07:47,140 --> 00:07:51,765
this observer or the author
of this sentence.

143
00:07:51,765 --> 00:07:53,895
What's the intention
of saying that?

144
00:07:53,895 --> 00:07:57,210
What's scenarios? What kind
of actions would be made?

145
00:07:57,210 --> 00:08:02,740
So this is another level

146
00:08:02,740 --> 00:08:05,755
of analysis that would
be very interesting.

147
00:08:05,755 --> 00:08:10,250
So this picture shows
that if we move down,

148
00:08:10,250 --> 00:08:12,530
we generally see
more sophisticated

149
00:08:12,530 --> 00:08:15,535
natural language processing
techniques to be used.

150
00:08:15,535 --> 00:08:18,080
Unfortunately,
such techniques would

151
00:08:18,080 --> 00:08:20,330
require more human effort,

152
00:08:20,330 --> 00:08:23,060
and they are less accurate.

153
00:08:23,060 --> 00:08:26,570
That means there are mistakes.

154
00:08:26,570 --> 00:08:29,945
So if we add an texts that are at

155
00:08:29,945 --> 00:08:32,240
the levels that are

156
00:08:32,240 --> 00:08:34,970
representing deeper
analysis of language,

157
00:08:34,970 --> 00:08:37,790
then we have to
tolerate the errors.

158
00:08:37,790 --> 00:08:42,380
So that also means it's
still necessary to combine

159
00:08:42,380 --> 00:08:46,835
such deep analysis with
shallow analysis based on,

160
00:08:46,835 --> 00:08:48,695
for example, sequence of words.

161
00:08:48,695 --> 00:08:50,675
On the right side,

162
00:08:50,675 --> 00:08:55,240
you'll see the arrow points
down to indicate that.

163
00:08:55,240 --> 00:08:56,960
As we go down,

164
00:08:56,960 --> 00:08:59,780
we are representation
of text is closer

165
00:08:59,780 --> 00:09:02,600
to knowledge representation
in our mind,

166
00:09:02,600 --> 00:09:08,210
and need for solving
a lot of problems.

167
00:09:08,210 --> 00:09:11,750
Now this is desirable because as

168
00:09:11,750 --> 00:09:15,110
we can represent text at
the level of knowledge,

169
00:09:15,110 --> 00:09:17,315
we can easily extract
the knowledge.

170
00:09:17,315 --> 00:09:19,280
That's the purpose
of text-mining.

171
00:09:19,280 --> 00:09:21,965
So there is a trade-off

172
00:09:21,965 --> 00:09:24,920
here between doing
a deeper analysis that

173
00:09:24,920 --> 00:09:27,260
might have errors but would give

174
00:09:27,260 --> 00:09:31,225
us direct knowledge that
can be extracted from text.

175
00:09:31,225 --> 00:09:33,910
Doing shallow analysis, which

176
00:09:33,910 --> 00:09:37,010
is more robust but
wouldn't actually

177
00:09:37,010 --> 00:09:42,665
give us the necessary deeper
representation of knowledge.

178
00:09:42,665 --> 00:09:45,740
I should also say that
text data are generated by

179
00:09:45,740 --> 00:09:49,085
humans and are meant to
be consumed by humans.

180
00:09:49,085 --> 00:09:52,340
So as a result, in
text data analysis,

181
00:09:52,340 --> 00:09:56,090
text-mining humans play
a very important role,

182
00:09:56,090 --> 00:09:58,010
they are always in the loop.

183
00:09:58,010 --> 00:10:00,650
Meaning that we should optimize

184
00:10:00,650 --> 00:10:03,695
the collaboration of
humans and computers.

185
00:10:03,695 --> 00:10:05,540
So in that sense,

186
00:10:05,540 --> 00:10:08,480
it's okay that computers
may not be able

187
00:10:08,480 --> 00:10:12,920
to have compute accurately
representation of text data,

188
00:10:12,920 --> 00:10:15,290
and the patterns
that are extracted

189
00:10:15,290 --> 00:10:18,035
from text data can be
interpreted by humans,

190
00:10:18,035 --> 00:10:20,840
and humans can
guide the computers

191
00:10:20,840 --> 00:10:24,650
to do more accurate analysis
by annotating more data,

192
00:10:24,650 --> 00:10:28,640
by providing features
to guide a machine

193
00:10:28,640 --> 00:10:33,870
learning programs to make
them work more effectively.